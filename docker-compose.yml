version: '3.8'

services:
  # HDFS NameNode
  namenode:
    image: apache/hadoop:3.3.6
    hostname: namenode
    command: ["hdfs", "namenode"]
    environment:
      - CLUSTER_NAME=hadoop-cluster
      - CORE_CONF_fs_defaultFS=hdfs://namenode:8020
      - CORE_CONF_hadoop_http_staticuser_user=root
      - CORE_CONF_hadoop_proxyuser_hue_hosts=*
      - CORE_CONF_hadoop_proxyuser_hue_groups=*
      - HDFS_CONF_dfs_webhdfs_enabled=true
      - HDFS_CONF_dfs_permissions_enabled=false
      - HDFS_CONF_dfs_namenode_datanode_registration_ip___hostname___check=false
      - HDFS_CONF_dfs_replication=2
      - HDFS_CONF_dfs_namenode_name_dir=/hadoop/dfs/name
      - HDFS_CONF_dfs_namenode_http_address=0.0.0.0:9870
    ports:
      - "9870:9870"
      - "8020:8020"
    volumes:
      - namenode_data:/hadoop/dfs/name
      - ./configs:/opt/hadoop/etc/hadoop:ro
    networks:
      - spark-net
    deploy:
      placement:
        constraints:
          - node.labels.role == master
      restart_policy:
        condition: on-failure
        delay: 15s
        max_attempts: 3

  # HDFS DataNodes
  datanode:
    image: apache/hadoop:3.3.6
    command: ["hdfs", "datanode"]
    environment:
      - CLUSTER_NAME=hadoop-cluster
      - CORE_CONF_fs_defaultFS=hdfs://namenode:8020
      - CORE_CONF_hadoop_http_staticuser_user=root
      - HDFS_CONF_dfs_webhdfs_enabled=true
      - HDFS_CONF_dfs_permissions_enabled=false
      - HDFS_CONF_dfs_namenode_datanode_registration_ip___hostname___check=false
      - HDFS_CONF_dfs_replication=2
      - HDFS_CONF_dfs_datanode_data_dir=/hadoop/dfs/data
      - HDFS_CONF_dfs_datanode_http_address=0.0.0.0:9864
      - HDFS_CONF_dfs_datanode_address=0.0.0.0:9866
      - HDFS_CONF_dfs_datanode_ipc_address=0.0.0.0:9867
    ports:
      - "9864"
      - "9866"
      - "9867"
    volumes:
      - datanode_data:/hadoop/dfs/data
      - ./configs:/opt/hadoop/etc/hadoop:ro
    networks:
      - spark-net
    depends_on:
      - namenode
    deploy:
      replicas: 4
      placement:
        constraints:
          - node.labels.role == worker
        preferences:
          - spread: node.id
      restart_policy:
        condition: on-failure
        delay: 20s
        max_attempts: 5

  # YARN Resource Manager
  yarn-resourcemanager:
    image: apache/hadoop:3.3.6
    hostname: yarn-resourcemanager
    command: ["yarn", "resourcemanager"]
    environment:
      - CLUSTER_NAME=hadoop-cluster
      - CORE_CONF_fs_defaultFS=hdfs://namenode:8020
      - YARN_CONF_yarn_log___aggregation___enable=true
      - YARN_CONF_yarn_log_server_url=http://yarn-timelineserver:8188/applicationhistory/logs/
      - YARN_CONF_yarn_resourcemanager_recovery_enabled=true
      - YARN_CONF_yarn_resourcemanager_store_class=org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore
      - YARN_CONF_yarn_resourcemanager_hostname=yarn-resourcemanager
      - YARN_CONF_yarn_resourcemanager_address=yarn-resourcemanager:8032
      - YARN_CONF_yarn_resourcemanager_scheduler_address=yarn-resourcemanager:8030
      - YARN_CONF_yarn_resourcemanager_resource___tracker_address=yarn-resourcemanager:8031
      - YARN_CONF_yarn_resourcemanager_webapp_address=0.0.0.0:8088
      - YARN_CONF_yarn_timeline___service_enabled=true
      - YARN_CONF_yarn_timeline___service_generic___application___history_enabled=true
      - YARN_CONF_mapreduce_map_output_compress=true
      - YARN_CONF_mapred_map_output_compress_codec=org.apache.hadoop.io.compress.SnappyCodec
      - YARN_CONF_yarn_nodemanager_resource_memory___mb=4096
      - YARN_CONF_yarn_scheduler_maximum___allocation___mb=4096
      - YARN_CONF_yarn_scheduler_minimum___allocation___mb=64
      - YARN_CONF_yarn_nodemanager_vmem___check___enabled=false
    ports:
      - "8088:8088"
      - "8032:8032"
      - "8030:8030"
      - "8031:8031"
    volumes:
      - ./configs:/opt/hadoop/etc/hadoop:ro
    networks:
      - spark-net
    depends_on:
      - namenode
    deploy:
      placement:
        constraints:
          - node.labels.role == master
      restart_policy:
        condition: on-failure
        delay: 25s

  # YARN Node Managers
  yarn-nodemanager:
    image: apache/hadoop:3.3.6
    command: ["yarn", "nodemanager"]
    environment:
      - CLUSTER_NAME=hadoop-cluster
      - CORE_CONF_fs_defaultFS=hdfs://namenode:8020
      - YARN_CONF_yarn_log___aggregation___enable=true
      - YARN_CONF_yarn_log_server_url=http://yarn-timelineserver:8188/applicationhistory/logs/
      - YARN_CONF_yarn_resourcemanager_hostname=yarn-resourcemanager
      - YARN_CONF_yarn_resourcemanager_address=yarn-resourcemanager:8032
      - YARN_CONF_yarn_resourcemanager_scheduler_address=yarn-resourcemanager:8030
      - YARN_CONF_yarn_resourcemanager_resource___tracker_address=yarn-resourcemanager:8031
      - YARN_CONF_yarn_nodemanager_aux___services=mapreduce_shuffle
      - YARN_CONF_yarn_nodemanager_aux___services_mapreduce___shuffle_class=org.apache.hadoop.mapred.ShuffleHandler
      - YARN_CONF_yarn_nodemanager_resource_memory___mb=2048
      - YARN_CONF_yarn_nodemanager_resource_cpu___vcores=2
      - YARN_CONF_yarn_nodemanager_disk___health___checker_max___disk___utilization___per___disk___percentage=98.5
      - YARN_CONF_yarn_nodemanager_remote___app___log___dir=/app-logs
      - YARN_CONF_yarn_nodemanager_webapp_address=0.0.0.0:8042
      - YARN_CONF_mapreduce_map_output_compress=true
      - YARN_CONF_mapred_map_output_compress_codec=org.apache.hadoop.io.compress.SnappyCodec
      - YARN_CONF_yarn_nodemanager_vmem___check___enabled=false
    ports:
      - "8042"
    volumes:
      - ./configs:/opt/hadoop/etc/hadoop:ro
      - yarn_logs:/opt/hadoop/logs/userlogs
    networks:
      - spark-net
    depends_on:
      - yarn-resourcemanager
      - namenode
    deploy:
      replicas: 4
      placement:
        constraints:
          - node.labels.role == worker
        preferences:
          - spread: node.id
      restart_policy:
        condition: on-failure
        delay: 30s

  # MapReduce History Server
  mapreduce-historyserver:
    image: apache/hadoop:3.3.6
    hostname: mapreduce-historyserver
    command: ["mapred", "historyserver"]
    environment:
      - CLUSTER_NAME=hadoop-cluster
      - CORE_CONF_fs_defaultFS=hdfs://namenode:8020
      - YARN_CONF_yarn_resourcemanager_hostname=yarn-resourcemanager
      - YARN_CONF_yarn_log___aggregation___enable=true
      - YARN_CONF_yarn_timeline___service_enabled=true
      - YARN_CONF_mapreduce_jobhistory_address=mapreduce-historyserver:10020
      - YARN_CONF_mapreduce_jobhistory_webapp_address=0.0.0.0:19888
      - YARN_CONF_mapreduce_jobhistory_intermediate___done___dir=/mr-history/tmp
      - YARN_CONF_mapreduce_jobhistory_done___dir=/mr-history/done
    ports:
      - "19888:19888"
      - "10020:10020"
    volumes:
      - ./configs:/opt/hadoop/etc/hadoop:ro
    networks:
      - spark-net
    depends_on:
      - namenode
      - yarn-resourcemanager
    deploy:
      placement:
        constraints:
          - node.labels.role == master
      restart_policy:
        condition: on-failure
        delay: 35s

  # Spark Master
  spark-master:
    image: bitnami/spark:3.5.0
    hostname: spark-master
    environment:
      - SPARK_MODE=master
      - SPARK_MASTER_HOST=spark-master
      - SPARK_MASTER_PORT_NUMBER=7077
      - SPARK_MASTER_WEBUI_PORT=8080
      - SPARK_DAEMON_MEMORY=1g
    ports:
      - "8080:8080"
      - "7077:7077"
    volumes:
      - ./configs/spark-defaults.conf:/opt/bitnami/spark/conf/spark-defaults.conf:ro
      - spark_events:/tmp/spark-events
    networks:
      - spark-net
    depends_on:
      - namenode
    deploy:
      placement:
        constraints:
          - node.labels.role == master
      restart_policy:
        condition: on-failure
        delay: 20s

  # Spark Workers
  spark-worker:
    image: bitnami/spark:3.5.0
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=2g
      - SPARK_WORKER_CORES=2
      - SPARK_WORKER_WEBUI_PORT=8081
      - SPARK_DAEMON_MEMORY=1g
    volumes:
      - ./configs/spark-defaults.conf:/opt/bitnami/spark/conf/spark-defaults.conf:ro
      - spark_events:/tmp/spark-events
    networks:
      - spark-net
    depends_on:
      - spark-master
      - namenode
    deploy:
      replicas: 4
      placement:
        constraints:
          - node.labels.role == worker
        preferences:
          - spread: node.id
      restart_policy:
        condition: on-failure
        delay: 25s
        max_attempts: 5

  # Spark History Server
  spark-history:
    image: bitnami/spark:3.5.0
    hostname: spark-history
    environment:
      - SPARK_MODE=history-server
      - SPARK_HISTORY_OPTS=-Dspark.history.fs.logDirectory=hdfs://namenode:8020/tmp/spark-events
    ports:
      - "18080:18080"
    volumes:
      - ./configs/spark-defaults.conf:/opt/bitnami/spark/conf/spark-defaults.conf:ro
      - spark_events:/tmp/spark-events
    networks:
      - spark-net
    depends_on:
      - namenode
      - spark-master
    deploy:
      placement:
        constraints:
          - node.labels.role == master
      restart_policy:
        condition: on-failure
        delay: 40s

volumes:
  namenode_data:
    driver: local
  datanode_data:
    driver: local
  spark_events:
    driver: local
  yarn_logs:
    driver: local

networks:
  spark-net:
    driver: overlay
    attachable: true